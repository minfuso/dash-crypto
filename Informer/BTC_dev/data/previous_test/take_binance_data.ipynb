{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45e7183f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "636fbfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL de base des archives Binance\n",
    "BASE_URL = \"https://data.binance.vision/data/spot/monthly/klines/BTCUSDT/1h/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fda86ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_month(year: int, month: int):\n",
    "    \"\"\"Télécharge les données de trading BTCUSDT pour un mois donné.\n",
    "    \n",
    "    Args:\n",
    "        year (int): Année (ex: 2023)\n",
    "        month (int): Mois (1-12)\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame contenant les données de trading, ou None si le fichier n'existe pas.\n",
    "    \"\"\"\n",
    "    fname = f\"BTCUSDT-1h-{year}-{month:02d}.zip\"\n",
    "    url = BASE_URL + fname\n",
    "    r = requests.get(url)\n",
    "    if r.status_code == 200:\n",
    "        z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "        csv_file = z.namelist()[0]\n",
    "        df = pd.read_csv(\n",
    "            z.open(csv_file), \n",
    "            header=None,\n",
    "            names=[\n",
    "                \"open_time\", \"open\", \"high\", \"low\", \"close\", \"volume\",\n",
    "                \"close_time\", \"quote_asset_volume\", \"number_of_trades\",\n",
    "                \"taker_buy_base\", \"taker_buy_quote\", \"ignore\"\n",
    "            ]\n",
    "        )\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"Pas trouvé : {url}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b1d09f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(start_year=2017, start_month=8, end_year=None, end_month=None, out_csv=\"BTCUSDT_1h.csv\"):\n",
    "    \"\"\"Télécharge et concatène toutes les données disponibles en un CSV unique.\"\"\"\n",
    "    if end_year is None or end_month is None:\n",
    "        now = datetime.utcnow()\n",
    "        end_year, end_month = now.year, now.month\n",
    "\n",
    "    dfs = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        for month in range(1, 13):\n",
    "            if (year == start_year and month < start_month) or (year == end_year and month > end_month):\n",
    "                continue\n",
    "            print(f\"Téléchargement {year}-{month:02d} ...\")\n",
    "            df = download_month(year, month)\n",
    "            if df is not None:\n",
    "                dfs.append(df)\n",
    "\n",
    "    if dfs:\n",
    "        all_data = pd.concat(dfs, ignore_index=True)\n",
    "        # Conversion timestamp en datetime lisible\n",
    "        all_data[\"open_time\"] = pd.to_datetime(all_data[\"open_time\"], unit=\"ms\", errors=\"coerce\")\n",
    "        all_data[\"close_time\"] = pd.to_datetime(all_data[\"close_time\"], unit=\"ms\", errors=\"coerce\")\n",
    "        \n",
    "        # Delete corrupted rows\n",
    "        # all_data.dropna(inplace=True, subset=[\"open_time\", \"close_time\"])\n",
    "        # Sauvegarde CSV\n",
    "        all_data.to_csv(out_csv, index=False)\n",
    "        print(f\"\\n✅ Dataset sauvegardé dans {out_csv} ({len(all_data)} lignes)\")\n",
    "        return all_data\n",
    "    else:\n",
    "        print(\"❌ Aucun fichier téléchargé.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97229600",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4026/123367092.py:4: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  now = datetime.utcnow()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Téléchargement 2017-08 ...\n",
      "Téléchargement 2017-09 ...\n",
      "Téléchargement 2017-10 ...\n",
      "Téléchargement 2017-11 ...\n",
      "Téléchargement 2017-12 ...\n",
      "Téléchargement 2018-01 ...\n",
      "Téléchargement 2018-02 ...\n",
      "Téléchargement 2018-03 ...\n",
      "Téléchargement 2018-04 ...\n",
      "Téléchargement 2018-05 ...\n",
      "Téléchargement 2018-06 ...\n",
      "Téléchargement 2018-07 ...\n",
      "Téléchargement 2018-08 ...\n",
      "Téléchargement 2018-09 ...\n",
      "Téléchargement 2018-10 ...\n",
      "Téléchargement 2018-11 ...\n",
      "Téléchargement 2018-12 ...\n",
      "Téléchargement 2019-01 ...\n",
      "Téléchargement 2019-02 ...\n",
      "Téléchargement 2019-03 ...\n",
      "Téléchargement 2019-04 ...\n",
      "Téléchargement 2019-05 ...\n",
      "Téléchargement 2019-06 ...\n",
      "Téléchargement 2019-07 ...\n",
      "Téléchargement 2019-08 ...\n",
      "Téléchargement 2019-09 ...\n",
      "Téléchargement 2019-10 ...\n",
      "Téléchargement 2019-11 ...\n",
      "Téléchargement 2019-12 ...\n",
      "Téléchargement 2020-01 ...\n",
      "Téléchargement 2020-02 ...\n",
      "Téléchargement 2020-03 ...\n",
      "Téléchargement 2020-04 ...\n",
      "Téléchargement 2020-05 ...\n",
      "Téléchargement 2020-06 ...\n",
      "Téléchargement 2020-07 ...\n",
      "Téléchargement 2020-08 ...\n",
      "Téléchargement 2020-09 ...\n",
      "Téléchargement 2020-10 ...\n",
      "Téléchargement 2020-11 ...\n",
      "Téléchargement 2020-12 ...\n",
      "Téléchargement 2021-01 ...\n",
      "Téléchargement 2021-02 ...\n",
      "Téléchargement 2021-03 ...\n",
      "Téléchargement 2021-04 ...\n",
      "Téléchargement 2021-05 ...\n",
      "Téléchargement 2021-06 ...\n",
      "Téléchargement 2021-07 ...\n",
      "Téléchargement 2021-08 ...\n",
      "Téléchargement 2021-09 ...\n",
      "Téléchargement 2021-10 ...\n",
      "Téléchargement 2021-11 ...\n",
      "Téléchargement 2021-12 ...\n",
      "Téléchargement 2022-01 ...\n",
      "Téléchargement 2022-02 ...\n",
      "Téléchargement 2022-03 ...\n",
      "Téléchargement 2022-04 ...\n",
      "Téléchargement 2022-05 ...\n",
      "Téléchargement 2022-06 ...\n",
      "Téléchargement 2022-07 ...\n",
      "Téléchargement 2022-08 ...\n",
      "Téléchargement 2022-09 ...\n",
      "Téléchargement 2022-10 ...\n",
      "Téléchargement 2022-11 ...\n",
      "Téléchargement 2022-12 ...\n",
      "Téléchargement 2023-01 ...\n",
      "Téléchargement 2023-02 ...\n",
      "Téléchargement 2023-03 ...\n",
      "Téléchargement 2023-04 ...\n",
      "Téléchargement 2023-05 ...\n",
      "Téléchargement 2023-06 ...\n",
      "Téléchargement 2023-07 ...\n",
      "Téléchargement 2023-08 ...\n",
      "Téléchargement 2023-09 ...\n",
      "Téléchargement 2023-10 ...\n",
      "Téléchargement 2023-11 ...\n",
      "Téléchargement 2023-12 ...\n",
      "Téléchargement 2024-01 ...\n",
      "Téléchargement 2024-02 ...\n",
      "Téléchargement 2024-03 ...\n",
      "Téléchargement 2024-04 ...\n",
      "Téléchargement 2024-05 ...\n",
      "Téléchargement 2024-06 ...\n",
      "Téléchargement 2024-07 ...\n",
      "Téléchargement 2024-08 ...\n",
      "Téléchargement 2024-09 ...\n",
      "Téléchargement 2024-10 ...\n",
      "Téléchargement 2024-11 ...\n",
      "Téléchargement 2024-12 ...\n",
      "Téléchargement 2025-01 ...\n",
      "Téléchargement 2025-02 ...\n",
      "Téléchargement 2025-03 ...\n",
      "Téléchargement 2025-04 ...\n",
      "Téléchargement 2025-05 ...\n",
      "Téléchargement 2025-06 ...\n",
      "Téléchargement 2025-07 ...\n",
      "Téléchargement 2025-08 ...\n",
      "Téléchargement 2025-09 ...\n",
      "Pas trouvé : https://data.binance.vision/data/spot/monthly/klines/BTCUSDT/1h/BTCUSDT-1h-2025-09.zip\n",
      "\n",
      "✅ Dataset sauvegardé dans ../raw/BTCUSDT_1h_full.csv (70357 lignes)\n"
     ]
    }
   ],
   "source": [
    "# Exemple d’utilisation : télécharger toutes les données depuis août 2017\n",
    "df = build_dataset(start_year=2017, start_month=8, out_csv=\"../raw/BTCUSDT_1h_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2197500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de doublons dans open_time : 5831\n",
      "Nombre de doublons dans open_time : 0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 64526 entries, 0 to 64525\n",
      "Data columns (total 12 columns):\n",
      " #   Column              Non-Null Count  Dtype         \n",
      "---  ------              --------------  -----         \n",
      " 0   open_time           64525 non-null  datetime64[ns]\n",
      " 1   open                64526 non-null  float64       \n",
      " 2   high                64526 non-null  float64       \n",
      " 3   low                 64526 non-null  float64       \n",
      " 4   close               64526 non-null  float64       \n",
      " 5   volume              64526 non-null  float64       \n",
      " 6   close_time          64525 non-null  datetime64[ns]\n",
      " 7   quote_asset_volume  64526 non-null  float64       \n",
      " 8   number_of_trades    64526 non-null  int64         \n",
      " 9   taker_buy_base      64526 non-null  float64       \n",
      " 10  taker_buy_quote     64526 non-null  float64       \n",
      " 11  ignore              64526 non-null  float64       \n",
      "dtypes: datetime64[ns](2), float64(9), int64(1)\n",
      "memory usage: 6.4 MB\n"
     ]
    }
   ],
   "source": [
    "duplicates = df[\"open_time\"].duplicated().sum()\n",
    "print(f\"Nombre de doublons dans open_time : {duplicates}\")\n",
    "\n",
    "df = df.drop_duplicates(subset=[\"open_time\"], keep=\"first\")\n",
    "\n",
    "duplicates = df[\"open_time\"].duplicated().sum()\n",
    "print(f\"Nombre de doublons dans open_time : {duplicates}\")\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee699f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 64652 entries, 2017-08-17 04:00:00 to 2024-12-31 23:00:00\n",
      "Freq: h\n",
      "Data columns (total 11 columns):\n",
      " #   Column              Non-Null Count  Dtype         \n",
      "---  ------              --------------  -----         \n",
      " 0   open                64482 non-null  float64       \n",
      " 1   high                64482 non-null  float64       \n",
      " 2   low                 64482 non-null  float64       \n",
      " 3   close               64482 non-null  float64       \n",
      " 4   volume              64482 non-null  float64       \n",
      " 5   close_time          64482 non-null  datetime64[ns]\n",
      " 6   quote_asset_volume  64482 non-null  float64       \n",
      " 7   number_of_trades    64482 non-null  float64       \n",
      " 8   taker_buy_base      64482 non-null  float64       \n",
      " 9   taker_buy_quote     64482 non-null  float64       \n",
      " 10  ignore              64482 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(10)\n",
      "memory usage: 5.9 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4026/2554304782.py:3: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  full_range = pd.date_range(df.index.min(), df.index.max(), freq=\"H\")\n"
     ]
    }
   ],
   "source": [
    "df = df.set_index(\"open_time\").sort_index()\n",
    "\n",
    "full_range = pd.date_range(df.index.min(), df.index.max(), freq=\"H\")\n",
    "df = df.reindex(full_range)\n",
    "\n",
    "df.index.name = \"open_time\"\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba980f49",
   "metadata": {},
   "source": [
    "## Add and transform for usefull features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6c1cce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rsi(series, window=14):\n",
    "    delta = series.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).fillna(0)\n",
    "    loss = (-delta.where(delta < 0, 0)).fillna(0)\n",
    "\n",
    "    avg_gain = gain.rolling(window=window).mean()\n",
    "    avg_loss = loss.rolling(window=window).mean()\n",
    "\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "def compute_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # SMA\n",
    "    df[\"sma_7d\"] = df[\"close\"].rolling(window=7*24).mean()\n",
    "    df[\"sma_30d\"] = df[\"close\"].rolling(window=30*24).mean()\n",
    "    df[\"sma_50d\"] = df[\"close\"].rolling(window=50*24).mean()\n",
    "    df[\"sma_100d\"] = df[\"close\"].rolling(window=100*24).mean()\n",
    "    \n",
    "    # Volatibility\n",
    "    df[\"return\"] = df[\"close\"].pct_change()\n",
    "\n",
    "    # Volatility on 20 hours\n",
    "    df[\"volatility_20\"] = df[\"return\"].rolling(window=20).std()\n",
    "    df[\"volatility_50\"] = df[\"return\"].rolling(window=50).std()\n",
    "    df[\"volatility_100\"] = df[\"return\"].rolling(window=100).std()\n",
    "    df[\"volatility_14d\"] = df[\"return\"].rolling(window=14*24).std()\n",
    "    \n",
    "    # RSI 14 and 14 days\n",
    "    df[\"rsi_14\"] = compute_rsi(df[\"close\"], window=14)\n",
    "    df[\"rsi_14d\"] = compute_rsi(df[\"close\"], window=14*24)\n",
    "    \n",
    "    # MACD\n",
    "    df = compute_MACD(df)\n",
    "    \n",
    "    # Relative volume 20\n",
    "    df[\"volume_sma20\"] = df[\"volume\"].rolling(window=20).mean()\n",
    "    df[\"volume_sma20d\"] = df[\"volume\"].rolling(window=20*24).mean()\n",
    "    df[\"volume_rel20\"] = df[\"volume\"] / df[\"volume_sma20\"]\n",
    "    df[\"volume_rel20d\"] = df[\"volume\"] / df[\"volume_sma20d\"]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def compute_MACD(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # EMA 12 et EMA 26\n",
    "    df[\"ema_12d\"] = df[\"close\"].ewm(span=12*24, adjust=False).mean()\n",
    "    df[\"ema_26d\"] = df[\"close\"].ewm(span=26*24, adjust=False).mean()\n",
    "    \n",
    "    # MACD line\n",
    "    df[\"MACD\"] = df[\"ema_12d\"] - df[\"ema_26d\"]\n",
    "\n",
    "    # Signal line (EMA 9 du MACD)\n",
    "    df[\"Signal\"] = df[\"MACD\"].ewm(span=9*24, adjust=False).mean()\n",
    "\n",
    "    # Histogramme\n",
    "    df[\"MACD_Hist\"] = df[\"MACD\"] - df[\"Signal\"]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c3d767c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 64652 entries, 2017-08-17 04:00:00 to 2024-12-31 23:00:00\n",
      "Freq: h\n",
      "Data columns (total 31 columns):\n",
      " #   Column              Non-Null Count  Dtype         \n",
      "---  ------              --------------  -----         \n",
      " 0   open                64482 non-null  float64       \n",
      " 1   high                64482 non-null  float64       \n",
      " 2   low                 64482 non-null  float64       \n",
      " 3   close               64482 non-null  float64       \n",
      " 4   volume              64482 non-null  float64       \n",
      " 5   close_time          64482 non-null  datetime64[ns]\n",
      " 6   quote_asset_volume  64482 non-null  float64       \n",
      " 7   number_of_trades    64482 non-null  float64       \n",
      " 8   taker_buy_base      64482 non-null  float64       \n",
      " 9   taker_buy_quote     64482 non-null  float64       \n",
      " 10  ignore              64482 non-null  float64       \n",
      " 11  sma_7d              59926 non-null  float64       \n",
      " 12  sma_30d             48125 non-null  float64       \n",
      " 13  sma_50d             40084 non-null  float64       \n",
      " 14  sma_100d            27203 non-null  float64       \n",
      " 15  return              64651 non-null  float64       \n",
      " 16  volatility_20       64632 non-null  float64       \n",
      " 17  volatility_50       64602 non-null  float64       \n",
      " 18  volatility_100      64552 non-null  float64       \n",
      " 19  volatility_14d      64316 non-null  float64       \n",
      " 20  rsi_14              64576 non-null  float64       \n",
      " 21  rsi_14d             64317 non-null  float64       \n",
      " 22  ema_12d             64652 non-null  float64       \n",
      " 23  ema_26d             64652 non-null  float64       \n",
      " 24  MACD                64652 non-null  float64       \n",
      " 25  Signal              64652 non-null  float64       \n",
      " 26  MACD_Hist           64652 non-null  float64       \n",
      " 27  volume_sma20        63931 non-null  float64       \n",
      " 28  volume_sma20d       52699 non-null  float64       \n",
      " 29  volume_rel20        63931 non-null  float64       \n",
      " 30  volume_rel20d       52699 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(30)\n",
      "memory usage: 15.8 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4026/768829287.py:23: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  df[\"return\"] = df[\"close\"].pct_change()\n"
     ]
    }
   ],
   "source": [
    "df = compute_features(df)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a74deff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Colonnes inutiles\n",
    "    drop_cols = [\n",
    "        \"close_time\", \"ignore\", \n",
    "        \"ema_12d\", \"ema_26d\", \n",
    "        \"volume_sma20\", \"volume_sma20d\"\n",
    "    ]\n",
    "    \n",
    "    df = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
    "    \n",
    "    # Suppression des lignes avec NaN restants\n",
    "    df = df.dropna()\n",
    "    \n",
    "    print(f\"✅ Dataset nettoyé : {df.shape[0]} lignes, {df.shape[1]} colonnes\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b69db15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset nettoyé : 27203 lignes, 25 colonnes\n"
     ]
    }
   ],
   "source": [
    "cleaned_df = clean_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d08dd76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 27203 entries, 2017-12-15 22:00:00 to 2024-12-31 23:00:00\n",
      "Data columns (total 25 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   open                27203 non-null  float64\n",
      " 1   high                27203 non-null  float64\n",
      " 2   low                 27203 non-null  float64\n",
      " 3   close               27203 non-null  float64\n",
      " 4   volume              27203 non-null  float64\n",
      " 5   quote_asset_volume  27203 non-null  float64\n",
      " 6   number_of_trades    27203 non-null  float64\n",
      " 7   taker_buy_base      27203 non-null  float64\n",
      " 8   taker_buy_quote     27203 non-null  float64\n",
      " 9   sma_7d              27203 non-null  float64\n",
      " 10  sma_30d             27203 non-null  float64\n",
      " 11  sma_50d             27203 non-null  float64\n",
      " 12  sma_100d            27203 non-null  float64\n",
      " 13  return              27203 non-null  float64\n",
      " 14  volatility_20       27203 non-null  float64\n",
      " 15  volatility_50       27203 non-null  float64\n",
      " 16  volatility_100      27203 non-null  float64\n",
      " 17  volatility_14d      27203 non-null  float64\n",
      " 18  rsi_14              27203 non-null  float64\n",
      " 19  rsi_14d             27203 non-null  float64\n",
      " 20  MACD                27203 non-null  float64\n",
      " 21  Signal              27203 non-null  float64\n",
      " 22  MACD_Hist           27203 non-null  float64\n",
      " 23  volume_rel20        27203 non-null  float64\n",
      " 24  volume_rel20d       27203 non-null  float64\n",
      "dtypes: float64(25)\n",
      "memory usage: 5.4 MB\n"
     ]
    }
   ],
   "source": [
    "cleaned_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05aeeefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df.to_parquet(\"../cleaned/BTC_features_clean.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7e68a6",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Treatmennt of the times series gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04ce2113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing the latences between hours in the close and open prices\n",
    "def fix_gaps(df:pd.DataFrame) -> list[pd.DataFrame]:\n",
    "    \"\"\"Split a time-indexed DataFrame into continuous segments where hourly gaps are detected.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): A DataFrame with a sorted DatetimeIndex (assumed hourly frequency),\n",
    "            typically containing OHLC data, in which we want to detect and isolate time gaps\n",
    "            longer than one hour.\n",
    "\n",
    "    Returns:\n",
    "        list[pd.DataFrame]: A list of DataFrames, each representing a continuous time series\n",
    "            segment with no gaps exceeding one hour. Suitable for independent processing or\n",
    "            analysis without temporal discontinuities.\n",
    "    \"\"\"\n",
    "    # Supposons que sub_df est votre DataFrame avec un DatetimeIndex trié\n",
    "    df = df.sort_index()\n",
    "\n",
    "    # Étape 1 : détecter les gaps > 1h\n",
    "    delta = df.index.to_series().diff()\n",
    "    is_gap = delta > pd.Timedelta('1h')\n",
    "\n",
    "    # Étape 2 : créer un groupe cumulatif avec cumsum()\n",
    "    group_id = is_gap.cumsum()\n",
    "\n",
    "    # Étape 3 : découper en sous-DataFrames\n",
    "    segments = [g for _, g in df.groupby(group_id)]\n",
    "\n",
    "    print(f\"Nombre de segments identifiés : {len(segments)}\")\n",
    "    \n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84415ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de segments identifiés : 8\n",
      "                         open      high       low     close       volume  \\\n",
      "open_time                                                                  \n",
      "2018-01-03 23:00:00  14792.32  14940.82  14710.00  14919.51   648.924200   \n",
      "2018-01-04 00:00:00  14919.51  14948.00  14710.25  14753.76   820.846740   \n",
      "2018-01-04 01:00:00  14765.65  15179.00  14649.99  15172.12  1181.627263   \n",
      "2018-01-04 02:00:00  15175.00  15280.00  15059.00  15110.90  1055.426462   \n",
      "2018-01-04 03:00:00  15110.90  15119.98  15110.90  15119.97     4.635181   \n",
      "\n",
      "                     quote_asset_volume  number_of_trades  taker_buy_base  \\\n",
      "open_time                                                                   \n",
      "2018-01-03 23:00:00        9.593982e+06            6624.0      527.040262   \n",
      "2018-01-04 00:00:00        1.214946e+07            6940.0      608.858344   \n",
      "2018-01-04 01:00:00        1.752356e+07            9267.0      844.362806   \n",
      "2018-01-04 02:00:00        1.605312e+07            9660.0      583.762400   \n",
      "2018-01-04 03:00:00        7.005581e+04              34.0        1.445181   \n",
      "\n",
      "                     taker_buy_quote        sma_7d  ...  volatility_50  \\\n",
      "open_time                                           ...                  \n",
      "2018-01-03 23:00:00     7.792466e+06  13801.199048  ...       0.014812   \n",
      "2018-01-04 00:00:00     9.013430e+06  13798.667619  ...       0.014919   \n",
      "2018-01-04 01:00:00     1.252303e+07  13797.311190  ...       0.015338   \n",
      "2018-01-04 02:00:00     8.880633e+06  13804.982440  ...       0.014932   \n",
      "2018-01-04 03:00:00     2.184474e+04  13810.676190  ...       0.014817   \n",
      "\n",
      "                     volatility_100  volatility_14d     rsi_14    rsi_14d  \\\n",
      "open_time                                                                   \n",
      "2018-01-03 23:00:00        0.015261        0.024443  59.334841  49.047949   \n",
      "2018-01-04 00:00:00        0.015157        0.024428  56.652761  48.754683   \n",
      "2018-01-04 01:00:00        0.015387        0.024471  59.267287  49.116996   \n",
      "2018-01-04 02:00:00        0.015380        0.024470  65.254261  49.142567   \n",
      "2018-01-04 03:00:00        0.015240        0.024438  57.483068  48.915479   \n",
      "\n",
      "                           MACD      Signal   MACD_Hist  volume_rel20  \\\n",
      "open_time                                                               \n",
      "2018-01-03 23:00:00  122.753369  471.660176 -348.906807      1.029889   \n",
      "2018-01-04 00:00:00  124.204991  468.457824 -344.252833      1.298638   \n",
      "2018-01-04 01:00:00  127.195676  465.312551 -338.116874      1.823090   \n",
      "2018-01-04 02:00:00  129.925581  462.221427 -332.295846      1.610153   \n",
      "2018-01-04 03:00:00  132.658785  459.183983 -326.525198      0.007444   \n",
      "\n",
      "                     volume_rel20d  \n",
      "open_time                           \n",
      "2018-01-03 23:00:00       0.976245  \n",
      "2018-01-04 00:00:00       1.233304  \n",
      "2018-01-04 01:00:00       1.771221  \n",
      "2018-01-04 02:00:00       1.579966  \n",
      "2018-01-04 03:00:00       0.006951  \n",
      "\n",
      "[5 rows x 25 columns]\n",
      "                        open     high      low    close       volume  \\\n",
      "open_time                                                              \n",
      "2018-05-22 03:00:00  8292.30  8326.65  8273.18  8324.94   658.572063   \n",
      "2018-05-22 04:00:00  8324.94  8326.64  8271.63  8279.00   945.930204   \n",
      "2018-05-22 05:00:00  8279.00  8297.99  8269.20  8287.00   534.149303   \n",
      "2018-05-22 06:00:00  8287.00  8368.34  8270.00  8327.10  1094.205516   \n",
      "2018-05-22 07:00:00  8327.10  8335.99  8303.00  8322.00   578.701769   \n",
      "\n",
      "                     quote_asset_volume  number_of_trades  taker_buy_base  \\\n",
      "open_time                                                                   \n",
      "2018-05-22 03:00:00        5.468709e+06            4310.0      393.673933   \n",
      "2018-05-22 04:00:00        7.843997e+06            5890.0      459.122009   \n",
      "2018-05-22 05:00:00        4.425676e+06            3719.0      297.555453   \n",
      "2018-05-22 06:00:00        9.115339e+06            8415.0      665.133132   \n",
      "2018-05-22 07:00:00        4.812311e+06            4719.0      392.970037   \n",
      "\n",
      "                     taker_buy_quote       sma_7d  ...  volatility_50  \\\n",
      "open_time                                          ...                  \n",
      "2018-05-22 03:00:00     3.269217e+06  8321.885893  ...       0.004415   \n",
      "2018-05-22 04:00:00     3.807675e+06  8319.404405  ...       0.004379   \n",
      "2018-05-22 05:00:00     2.465174e+06  8316.814940  ...       0.004341   \n",
      "2018-05-22 06:00:00     5.542183e+06  8314.390536  ...       0.004385   \n",
      "2018-05-22 07:00:00     3.267514e+06  8311.867321  ...       0.004383   \n",
      "\n",
      "                     volatility_100  volatility_14d     rsi_14    rsi_14d  \\\n",
      "open_time                                                                   \n",
      "2018-05-22 03:00:00        0.004948        0.007022  31.352053  46.015822   \n",
      "2018-05-22 04:00:00        0.004834        0.007028  34.714566  45.909709   \n",
      "2018-05-22 05:00:00        0.004829        0.007007  36.051184  46.257437   \n",
      "2018-05-22 06:00:00        0.004846        0.007013  43.209493  46.388135   \n",
      "2018-05-22 07:00:00        0.004847        0.007010  44.254518  46.256325   \n",
      "\n",
      "                           MACD     Signal   MACD_Hist  volume_rel20  \\\n",
      "open_time                                                              \n",
      "2018-05-22 03:00:00 -132.513642 -29.938553 -102.575088      0.722385   \n",
      "2018-05-22 04:00:00 -132.957074 -30.888033 -102.069041      1.022284   \n",
      "2018-05-22 05:00:00 -133.363320 -31.832506 -101.530815      0.582091   \n",
      "2018-05-22 06:00:00 -133.613322 -32.770578 -100.842745      1.213559   \n",
      "2018-05-22 07:00:00 -133.876815 -33.702432 -100.174382      0.658697   \n",
      "\n",
      "                     volume_rel20d  \n",
      "open_time                           \n",
      "2018-05-22 03:00:00       0.551175  \n",
      "2018-05-22 04:00:00       0.791489  \n",
      "2018-05-22 05:00:00       0.447180  \n",
      "2018-05-22 06:00:00       0.916588  \n",
      "2018-05-22 07:00:00       0.485201  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "# Separate whole data into different continuous segments\n",
    "continue_dfs = fix_gaps(cleaned_df)\n",
    "print(continue_dfs[0].tail())\n",
    "print(continue_dfs[1].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "910580d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reassign_close_to_open(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Reassign the 'open' price of each hour to be the 'close' price of the previous hour.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with OHLC data and a DatetimeIndex.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with updated 'open' prices.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"shifted_close\"] = df[\"close\"].shift(1)\n",
    "    df[\"open\"] = df[\"shifted_close\"]\n",
    "    df.drop(columns=[\"shifted_close\"], inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50014cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reassing close to open for each segment\n",
    "for i, df_elemen in enumerate(continue_dfs):\n",
    "    continue_dfs[i] = reassign_close_to_open(df_elemen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c69403a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Treatment of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ac09e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_future_evolution(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Define a boolean target indicating if the price will go up in the next hour.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with OHLC data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with an additional 'will_up' column.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"will_up\"] = (df[\"close\"].shift(-1) > df[\"close\"]).astype(bool)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd74218f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df_element in enumerate(continue_dfs):\n",
    "    continue_dfs[i] = define_future_evolution(df_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96c6d018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>will_up</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>open_time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-12-15 23:00:00</th>\n",
       "      <td>17502.72</td>\n",
       "      <td>17539.83</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-16 00:00:00</th>\n",
       "      <td>17539.83</td>\n",
       "      <td>17577.99</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-16 01:00:00</th>\n",
       "      <td>17577.99</td>\n",
       "      <td>17460.00</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-16 02:00:00</th>\n",
       "      <td>17460.00</td>\n",
       "      <td>17350.00</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-16 03:00:00</th>\n",
       "      <td>17350.00</td>\n",
       "      <td>17250.00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-16 04:00:00</th>\n",
       "      <td>17250.00</td>\n",
       "      <td>17437.06</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-16 05:00:00</th>\n",
       "      <td>17437.06</td>\n",
       "      <td>17489.00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-16 06:00:00</th>\n",
       "      <td>17489.00</td>\n",
       "      <td>17625.88</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-16 07:00:00</th>\n",
       "      <td>17625.88</td>\n",
       "      <td>17677.54</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-16 08:00:00</th>\n",
       "      <td>17677.54</td>\n",
       "      <td>17720.00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-16 09:00:00</th>\n",
       "      <td>17720.00</td>\n",
       "      <td>17788.00</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-16 10:00:00</th>\n",
       "      <td>17788.00</td>\n",
       "      <td>17775.39</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-16 11:00:00</th>\n",
       "      <td>17775.39</td>\n",
       "      <td>17829.99</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-16 12:00:00</th>\n",
       "      <td>17829.99</td>\n",
       "      <td>17919.00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-16 13:00:00</th>\n",
       "      <td>17919.00</td>\n",
       "      <td>18220.00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-16 14:00:00</th>\n",
       "      <td>18220.00</td>\n",
       "      <td>18540.00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-16 15:00:00</th>\n",
       "      <td>18540.00</td>\n",
       "      <td>18751.02</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-16 16:00:00</th>\n",
       "      <td>18751.02</td>\n",
       "      <td>18669.00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-16 17:00:00</th>\n",
       "      <td>18669.00</td>\n",
       "      <td>18700.00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-16 18:00:00</th>\n",
       "      <td>18700.00</td>\n",
       "      <td>18747.53</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         open     close  will_up\n",
       "open_time                                       \n",
       "2017-12-15 23:00:00  17502.72  17539.83     True\n",
       "2017-12-16 00:00:00  17539.83  17577.99    False\n",
       "2017-12-16 01:00:00  17577.99  17460.00    False\n",
       "2017-12-16 02:00:00  17460.00  17350.00    False\n",
       "2017-12-16 03:00:00  17350.00  17250.00     True\n",
       "2017-12-16 04:00:00  17250.00  17437.06     True\n",
       "2017-12-16 05:00:00  17437.06  17489.00     True\n",
       "2017-12-16 06:00:00  17489.00  17625.88     True\n",
       "2017-12-16 07:00:00  17625.88  17677.54     True\n",
       "2017-12-16 08:00:00  17677.54  17720.00     True\n",
       "2017-12-16 09:00:00  17720.00  17788.00    False\n",
       "2017-12-16 10:00:00  17788.00  17775.39     True\n",
       "2017-12-16 11:00:00  17775.39  17829.99     True\n",
       "2017-12-16 12:00:00  17829.99  17919.00     True\n",
       "2017-12-16 13:00:00  17919.00  18220.00     True\n",
       "2017-12-16 14:00:00  18220.00  18540.00     True\n",
       "2017-12-16 15:00:00  18540.00  18751.02    False\n",
       "2017-12-16 16:00:00  18751.02  18669.00     True\n",
       "2017-12-16 17:00:00  18669.00  18700.00     True\n",
       "2017-12-16 18:00:00  18700.00  18747.53     True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "continue_dfs[0][[\"open\", \"close\", \"will_up\"]].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe09495",
   "metadata": {},
   "source": [
    "---\n",
    "## Treating last segment to get validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "615f213d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows across all segments: 27195\n",
      "Row in last df: 13162\n",
      "ideal size of val and test set (15% each): 4079\n",
      "\n",
      "Size of val set: 4079\n",
      "Size of test set: 4079\n",
      "Size of reshaped last segment: 5004\n",
      "Size of all those segments: 13162\n",
      "\n",
      "Total size of all the segments after replacement: 27195\n"
     ]
    }
   ],
   "source": [
    "continue_dfs_copy = continue_dfs.copy()\n",
    "\n",
    "last_df = continue_dfs_copy[-1]\n",
    "total_len = sum(len(d) for d in continue_dfs_copy)\n",
    "print(f\"Total rows across all segments: {total_len}\")\n",
    "print(f\"Row in last df: {len(last_df)}\")\n",
    "\n",
    "ideal_size_valtest = int(total_len * 0.15)\n",
    "print(f\"ideal size of val and test set (15% each): {ideal_size_valtest}\")\n",
    "test_df = last_df.iloc[-ideal_size_valtest:]\n",
    "val_df = last_df.iloc[-2*ideal_size_valtest:-ideal_size_valtest] \n",
    "last_seg_reshaped = last_df.iloc[:-2*ideal_size_valtest]\n",
    "\n",
    "print(\"\")\n",
    "print(f\"Size of val set: {len(val_df)}\")\n",
    "print(f\"Size of test set: {len(test_df)}\")\n",
    "print(f\"Size of reshaped last segment: {len(last_seg_reshaped)}\")\n",
    "print(f\"Size of all those segments: {len(val_df) + len(test_df) + len(last_seg_reshaped)}\")\n",
    "\n",
    "\n",
    "# Replace last segment with reshaped one and add val and test sets\n",
    "continue_dfs_copy[-1] = last_seg_reshaped\n",
    "continue_dfs_copy.append(val_df)\n",
    "continue_dfs_copy.append(test_df)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Total size of all the segments after replacement:\" , sum(len(d) for d in continue_dfs_copy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "98bfdb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation of the changes\n",
    "\n",
    "continue_dfs = continue_dfs_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c5500e",
   "metadata": {},
   "source": [
    "---\n",
    "## Saving all the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79448151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_segments(segments: list[pd.DataFrame], base_filename=\"BTCUSDT_1h_continuous\") -> None:\n",
    "    \"\"\"Save each DataFrame segment to a separate CSV file. Last files will be named val and test respectively.\n",
    "\n",
    "    Args:\n",
    "        segments (list[pd.DataFrame]): List of DataFrame segments to save.\n",
    "        base_filename (str): Base filename for the output CSV files.\n",
    "    \"\"\"\n",
    "    for i, segment in enumerate(segments):\n",
    "        filename = f\"{base_filename}_{i+1}.csv\"\n",
    "        if i == len(segments) - 2:\n",
    "            filename = f\"{base_filename}_val.csv\"\n",
    "        elif i == len(segments) - 1:\n",
    "            filename = f\"{base_filename}_test.csv\"\n",
    "        segment.to_csv(filename)\n",
    "        print(f\"Segment {i+1} sauvegardé dans {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c53abdf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment 1 sauvegardé dans ../prepared/BTCUSDT_1h_continuous_1.csv\n",
      "Segment 2 sauvegardé dans ../prepared/BTCUSDT_1h_continuous_2.csv\n",
      "Segment 3 sauvegardé dans ../prepared/BTCUSDT_1h_continuous_3.csv\n",
      "Segment 4 sauvegardé dans ../prepared/BTCUSDT_1h_continuous_4.csv\n",
      "Segment 5 sauvegardé dans ../prepared/BTCUSDT_1h_continuous_5.csv\n",
      "Segment 6 sauvegardé dans ../prepared/BTCUSDT_1h_continuous_6.csv\n",
      "Segment 7 sauvegardé dans ../prepared/BTCUSDT_1h_continuous_7.csv\n",
      "Segment 8 sauvegardé dans ../prepared/BTCUSDT_1h_continuous_8.csv\n",
      "Segment 9 sauvegardé dans ../prepared/BTCUSDT_1h_continuous_val.csv\n",
      "Segment 10 sauvegardé dans ../prepared/BTCUSDT_1h_continuous_test.csv\n"
     ]
    }
   ],
   "source": [
    "save_segments(continue_dfs, base_filename=\"../prepared/BTCUSDT_1h_continuous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c849450d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dash-crypto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
