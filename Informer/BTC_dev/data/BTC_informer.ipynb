{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a8eaef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a906737c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 27203 entries, 2017-12-15 22:00:00 to 2024-12-31 23:00:00\n",
      "Data columns (total 25 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   open                27203 non-null  float64\n",
      " 1   high                27203 non-null  float64\n",
      " 2   low                 27203 non-null  float64\n",
      " 3   close               27203 non-null  float64\n",
      " 4   volume              27203 non-null  float64\n",
      " 5   quote_asset_volume  27203 non-null  float64\n",
      " 6   number_of_trades    27203 non-null  float64\n",
      " 7   taker_buy_base      27203 non-null  float64\n",
      " 8   taker_buy_quote     27203 non-null  float64\n",
      " 9   sma_7d              27203 non-null  float64\n",
      " 10  sma_30d             27203 non-null  float64\n",
      " 11  sma_50d             27203 non-null  float64\n",
      " 12  sma_100d            27203 non-null  float64\n",
      " 13  return              27203 non-null  float64\n",
      " 14  volatility_20       27203 non-null  float64\n",
      " 15  volatility_50       27203 non-null  float64\n",
      " 16  volatility_100      27203 non-null  float64\n",
      " 17  volatility_14d      27203 non-null  float64\n",
      " 18  rsi_14              27203 non-null  float64\n",
      " 19  rsi_14d             27203 non-null  float64\n",
      " 20  MACD                27203 non-null  float64\n",
      " 21  Signal              27203 non-null  float64\n",
      " 22  MACD_Hist           27203 non-null  float64\n",
      " 23  volume_rel20        27203 non-null  float64\n",
      " 24  volume_rel20d       27203 non-null  float64\n",
      "dtypes: float64(25)\n",
      "memory usage: 5.4 MB\n"
     ]
    }
   ],
   "source": [
    "# Treating data\n",
    "\n",
    "# Loading data\n",
    "df = pd.read_parquet(\"BTC_features_clean.parquet\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b3e95b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming data into inputs for the model\n",
    "\n",
    "def make_windows(df, input_len=168, output_horizons=[1, 6, 12, 24, 168], target_col=\"close\"):\n",
    "    \"\"\"\n",
    "    Génère les fenêtres (X, Y) pour l'entraînement d'un modèle séquentiel.\n",
    "    \n",
    "    df : DataFrame Pandas (index = datetime, colonnes = features)\n",
    "    input_len : longueur de la fenêtre d'entrée (en heures)\n",
    "    output_horizons : horizons de prédiction (en heures)\n",
    "    target_col : colonne de référence pour la target (ex. 'close')\n",
    "    \n",
    "    Retourne :\n",
    "        X : np.array (n_samples, input_len, n_features)\n",
    "        Y : np.array (n_samples, len(output_horizons))\n",
    "    \"\"\"\n",
    "    feature_cols = [c for c in df.columns if c != target_col]\n",
    "    data = df[feature_cols].values\n",
    "    target = df[target_col].values\n",
    "    n_features = data.shape[1]\n",
    "\n",
    "    X, Y = [], []\n",
    "    max_h = max(output_horizons)\n",
    "\n",
    "    for t in range(len(df) - input_len - max_h):\n",
    "        # Fenêtre d'entrée\n",
    "        x_window = data[t : t + input_len]\n",
    "\n",
    "        # Targets en rendements relatifs\n",
    "        y_window = []\n",
    "        current_price = target[t + input_len - 1]\n",
    "        for h in output_horizons:\n",
    "            future_price = target[t + input_len + h - 1]\n",
    "            y_window.append(future_price / current_price - 1)\n",
    "\n",
    "        X.append(x_window)\n",
    "        Y.append(y_window)\n",
    "\n",
    "    X = np.array(X, dtype=np.float32)\n",
    "    Y = np.array(Y, dtype=np.float32)\n",
    "\n",
    "    print(f\"✅ make_windows: X={X.shape}, Y={Y.shape}\")\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2b071ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ make_windows: X=(26867, 168, 24), Y=(26867, 5)\n"
     ]
    }
   ],
   "source": [
    "# Inputs and Outputs generations for the whole dataset\n",
    "X, Y = make_windows(df, input_len=168, output_horizons=[1, 6, 12, 24, 168], target_col=\"close\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b548abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset for train, validation and test.\n",
    "\n",
    "def time_series_split(X, Y, train_size=0.7, val_size=0.15):\n",
    "    \"\"\"\n",
    "    Split temporel des données en train, val et test.\n",
    "    \"\"\"\n",
    "    n = len(X)\n",
    "    train_end = int(n * train_size)\n",
    "    val_end = int(n * (train_size + val_size))\n",
    "    \n",
    "    X_train, Y_train = X[:train_end], Y[:train_end]\n",
    "    X_val,   Y_val   = X[train_end:val_end], Y[train_end:val_end]\n",
    "    X_test,  Y_test  = X[val_end:], Y[val_end:]\n",
    "    \n",
    "    print(f\"Train: {X_train.shape}, {Y_train.shape}\")\n",
    "    print(f\"Val:   {X_val.shape}, {Y_val.shape}\")\n",
    "    print(f\"Test:  {X_test.shape}, {Y_test.shape}\")\n",
    "    \n",
    "    return (X_train, Y_train), (X_val, Y_val), (X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8c294bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (18806, 168, 24), (18806, 5)\n",
      "Val:   (4030, 168, 24), (4030, 5)\n",
      "Test:  (4031, 168, 24), (4031, 5)\n"
     ]
    }
   ],
   "source": [
    "(X_train, Y_train), (X_val, Y_val), (X_test, Y_test) = time_series_split(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d86cc91",
   "metadata": {},
   "source": [
    "## INFORMER CREATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123c9806",
   "metadata": {},
   "source": [
    "### Etape 0: Normalization of the features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee4ec55",
   "metadata": {},
   "source": [
    "On utilise ici des moyennes sur le train car c'est la norme d'utilisation pour éviter le data leaking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b9cc91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (18806, 168, 24), (18806, 5)\n",
      "Val:   (4030, 168, 24), (4030, 5)\n",
      "Test:  (4031, 168, 24), (4031, 5)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Suppose que tu as déjà X, Y puis le split\n",
    "(X_train, Y_train), (X_val, Y_val), (X_test, Y_test) = time_series_split(X, Y)\n",
    "\n",
    "# Stats uniquement sur le train (pas de fuite d'info)\n",
    "feat_mean = X_train.mean(axis=(0,1), keepdims=True)\n",
    "feat_std  = X_train.std(axis=(0,1), keepdims=True) + 1e-8\n",
    "\n",
    "X_train_n = (X_train - feat_mean) / feat_std\n",
    "X_val_n   = (X_val   - feat_mean) / feat_std\n",
    "X_test_n  = (X_test  - feat_mean) / feat_std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec0ea28",
   "metadata": {},
   "source": [
    "### Etape 1 : Conding ProbSparse attention -- Multi-Head attention for Informers.\n",
    "\n",
    "On regarde chaque étape du call de la classe\n",
    "\n",
    "#### Etape 0 :\n",
    "\n",
    "On récupère dans `B` et `T` la taille du batch et la taille de la séquence respectivement.\n",
    "\n",
    "#### Etape 1 :\n",
    "\n",
    "On commence par projetter linéairement `X` sur `Q`, `K` et `V` à l'aide de différentes matrices. Mathématique, on a:\n",
    "$$\n",
    "Q = XW_Q, \\quad K = XW_K, \\quad V = XW_V\n",
    "$$\n",
    "\n",
    "X étant de shape `(B, T, d_mondel ou d_in)`, nous obtenons des objets `Q`, `K` et `V` de taille `(B, T, d_model)`.\n",
    "\n",
    "Les poids des matrices $W_Q$, $W_K$ et $W_V$ (de shape `(d_in, d_model)`) sont entraînables.\n",
    "\n",
    "#### Etape 2 : \n",
    "\n",
    "On divise notre attention en multi-tête, comme pour un transformer. Ce nombre de tête est défini par `num_head`. La valeur par défault de la classe est 4. Ainsi, on passe d'une information de taille `d_model` à `d_k` présent dans chacune des `num_head` têtes, avec `d_k` défini tel que:\n",
    "\n",
    "$$\n",
    "d_k = \\frac{d_{\\text{model}}}{\\text{num\\_heads}}\n",
    "$$\n",
    "\n",
    "La fonction `split_heads` prend en entrée un tenseur et fais cette séparation en `num_head` têtes de taille `d_k`. Ainsi, on l'applique à `Q`, `K` et `V`. Ils deviennent des objets de shape `(B, T, h, d_k)`.\n",
    "\n",
    "#### Etape 3: \n",
    "\n",
    "L'idée est de traîté chaque tête comme une séquence indépendante. Ainsi, on va applatir notre vecteur sur la dimension `B`et `h`, de manière à avoir un objet de shape `(B*h, T, d_k)`. On stocks ces nouveaux objets dans `Q_`, `K_` et `V_` pour respectivement les objets `Q`, `K` et `V`.\n",
    "\n",
    "#### Etape 4:\n",
    "\n",
    "D'abord, on applique une régularisation L2 permettant de réduire l'overfitting sur `Q_` et `K_`, que l'on stock respectivement dans `norm_q` et `norm_k`. Il est important que, dans ce cas, nous avons:\n",
    "$$\n",
    "||q_i|| = ||k_j|| = 1 \\,\n",
    "$$\n",
    "avec $||q_i||$ et $||k_j||$, les éléments $i$ et $j$ correspondant à la dimension `T` de `norm_q` et `norm_k` respectivement.\n",
    "\n",
    "Ensuite, nous pouvons calculer les scores pour les objets `norm_q` et `norm_k`. On nomme ceci `scores` car on regarde, pour chaque $q_i$, un score de similarité avec chaque $k_j$. Les scores sont calculés de cette manière :\n",
    "$$\n",
    "s_i = \\text{max} \\frac{q_i \\cdot k_j}{||q_i|| \\times ||k_j||}\n",
    "$$\n",
    "\n",
    "Dans notre code, nous calculons d'abord tous les $s_{i,j}$, et ensuite nous prenons le $s_i$ max sur pour chaque $j$ avec respectivement les variales `scores`et `max_scores`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b257b1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class ProbSparseAttention(layers.Layer):\n",
    "    \"\"\"\n",
    "    Version pédagogique de ProbSparse Self-Attention.\n",
    "    Sélectionne seulement les top-u queries les plus informatives.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads=4, dropout=0.1, u=10, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        assert d_model % num_heads == 0, \"d_model doit être divisible par num_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.dropout = dropout\n",
    "        self.u = u  # nombre de queries gardées\n",
    "\n",
    "        # Projections linéaires\n",
    "        self.W_q = layers.Dense(d_model)\n",
    "        self.W_k = layers.Dense(d_model)\n",
    "        self.W_v = layers.Dense(d_model)\n",
    "        self.W_o = layers.Dense(d_model)\n",
    "\n",
    "        self.drop = layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        \"\"\"\n",
    "        x: (batch, T, d_model)\n",
    "        \"\"\"\n",
    "        # 0. Récupération de la taille du batch B et de la séquence T\n",
    "        B, T, _ = tf.unstack(tf.shape(x))\n",
    "\n",
    "        # 1. Projeter en Q, K, V\n",
    "        Q = self.W_q(x)  # (B, T, d_model)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "\n",
    "        # 2. Découper en têtes\n",
    "        def split_heads(tensor):\n",
    "            return tf.reshape(tensor, (B, T, self.num_heads, self.d_k))  # (B, T, h, d_k)\n",
    "\n",
    "        Q = split_heads(Q)\n",
    "        K = split_heads(K)\n",
    "        V = split_heads(V)\n",
    "\n",
    "        # 3. Aplatir batch*têtes\n",
    "        Q_ = tf.reshape(Q, (B*self.num_heads, T, self.d_k)) # (B*h, T, d_k)\n",
    "        K_ = tf.reshape(K, (B*self.num_heads, T, self.d_k))\n",
    "        V_ = tf.reshape(V, (B*self.num_heads, T, self.d_k))\n",
    "\n",
    "        # 4. Calcul score de sparsité (max similitude)\n",
    "        norm_q = tf.nn.l2_normalize(Q_, axis=-1)\n",
    "        norm_k = tf.nn.l2_normalize(K_, axis=-1)\n",
    "        scores = tf.matmul(norm_q, norm_k, transpose_b=True)  # (B*h, T, T)\n",
    "        max_scores = tf.reduce_max(scores, axis=-1)  # (B*h, T)\n",
    "\n",
    "        # 5. Top-u queries\n",
    "        u = tf.minimum(self.u, T)\n",
    "        top_idx = tf.argsort(max_scores, axis=-1, direction=\"DESCENDING\")[:, :u]  # (B*h, u)\n",
    "\n",
    "        # 6. Extraire les queries sélectionnées\n",
    "        Q_sel = tf.gather(Q_, top_idx, batch_dims=1)  # (B*h, u, d_k)\n",
    "\n",
    "        # 7. Attention seulement sur Q_sel vs tous K,V\n",
    "        attn_weights = tf.matmul(Q_sel, K_, transpose_b=True) / tf.math.sqrt(tf.cast(self.d_k, tf.float32))\n",
    "        attn_weights = tf.nn.softmax(attn_weights, axis=-1)\n",
    "        attn_weights = self.drop(attn_weights, training=training)\n",
    "\n",
    "        out = tf.matmul(attn_weights, V_)  # (B*h, u, d_k)\n",
    "\n",
    "        # 8. Reconstruire la séquence complète (remplir les queries non sélectionnées par 0)\n",
    "        out_full = tf.zeros_like(Q_)  # (B*h, T, d_k)\n",
    "\n",
    "        # indices pour scatter update\n",
    "        batch_idx = tf.repeat(tf.range(B*self.num_heads)[:, None], u, axis=1)  # (B*h, u)\n",
    "        idx = tf.stack([batch_idx, top_idx], axis=-1)  # (B*h, u, 2)\n",
    "\n",
    "        out_full = tf.tensor_scatter_nd_update(out_full, tf.reshape(idx, (-1, 2)), tf.reshape(out, (-1, self.d_k)))\n",
    "\n",
    "        # 9. Reconstruire (B, T, d_model)\n",
    "        out_full = tf.reshape(out_full, (B, T, self.d_model))\n",
    "        return self.W_o(out_full)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda7bbc8",
   "metadata": {},
   "source": [
    "Ici, on fait comme avec le transformer. Se référer à la documentation. On code le **positional encoding**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a80238c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-03 15:14:20.686040: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class SinusoidalPE(layers.Layer):\n",
    "    \"\"\"Encodage positionnel sin/cos (Vaswani et al., 2017).\"\"\"\n",
    "    def __init__(self, d_model, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def call(self, x):\n",
    "        # x: (B, T, d_model) — B=batch, T=longueur séquence, d_model=dim embedding\n",
    "        T = tf.shape(x)[1]\n",
    "        d = self.d_model\n",
    "\n",
    "        # positions: [0, 1, ..., T-1]  shape (T, 1)\n",
    "        pos = tf.cast(tf.range(T)[:, None], tf.float32)\n",
    "\n",
    "        # indices de dimensions: [0, 1, ..., d-1] shape (1, d)\n",
    "        i = tf.cast(tf.range(d)[None, :], tf.float32)\n",
    "\n",
    "        # angle_rates = 1 / 10000^{2i/d}\n",
    "        angle_rates = tf.pow(10000.0, - (tf.floor(i/2.0) * 2.0) / tf.cast(d, tf.float32))\n",
    "        # angle_rads = pos * angle_rates  shape (T, d)\n",
    "        angle_rads = pos * angle_rates\n",
    "\n",
    "        # Appliquer sin sur dimensions paires (2k), cos sur impaires (2k+1)\n",
    "        sines = tf.sin(angle_rads[:, 0::2])\n",
    "        coses = tf.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        # Recomposer en (T, d): intercaler sin/cos\n",
    "        pe = tf.concat([tf.reshape(tf.stack([sines, coses], axis=-1), (T, -1)),\n",
    "                        tf.zeros((T, d - tf.shape(tf.reshape(tf.stack([sines, coses], axis=-1), (T, -1)))[1]))], axis=-1)\n",
    "        pe = pe[:, :d]  # sécurité si d est impair\n",
    "\n",
    "        return x + pe[None, ...]  # broadcast sur le batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9134bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dash-crypto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
